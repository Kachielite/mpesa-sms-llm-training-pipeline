{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **MPESA LLM Fine‑Tuning on Mac M1 (16GB): Step-by-Step Guide**\n",
    "\n",
    "This notebook is a practical, beginner-friendly guide for fine-tuning a Large Language Model (LLM) using MPESA SMS transaction data. Each step is clearly explained, with code and rationale, so you can follow along and understand the process from start to finish.\n",
    "\n",
    "**What you'll accomplish:**\n",
    "- Prepare and load your MPESA SMS dataset\n",
    "- Select and configure a base LLM\n",
    "- Apply LoRA (PEFT) for efficient fine-tuning\n",
    "- Set up and run supervised fine-tuning (SFT) with TRL\n",
    "- Save, push, and optionally merge your trained model\n",
    "- Run a quick inference to check your results\n",
    "\n",
    "**Workflow Overview:**\n",
    "1. Login to Hugging Face and Weights & Biases\n",
    "2. Load and Prepare the MPESA SMS Data\n",
    "3. Choose a Base Model\n",
    "4. Load Tokenizer and Model\n",
    "5. Configure LoRA (PEFT)\n",
    "6. Training Configuration (TRL SFT)\n",
    "7. Train-on-Answer (ToA)\n",
    "8. Fine-Tune (SFT Trainer)\n",
    "9. Save, Push to Hub, and Optionally Merge LoRA Weights\n",
    "10. Quick Sanity Check Inference"
   ],
   "id": "572da22976d2ff41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Login to Hugging Face and Weights & Biases**\n",
    "\n",
    "We'll start by logging into the Hugging Face Hub and Weights & Biases (WandB) for model management and experiment tracking. Make sure you have your API tokens ready."
   ],
   "id": "61f759ecd78669a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Check for .env file and load environment variables\n",
    "if not os.path.exists('../.env'):\n",
    "    print(\"Warning: .env file not found in the current directory.\")\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# 1. Login to Hugging Face (run this once per session)\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"Logged in to Hugging Face Hub.\")\n",
    "else:\n",
    "    raise ValueError(\"HF_TOKEN not set in .env file.\")\n",
    "\n",
    "# 2. Login to Weights & Biases (run this once per session)\n",
    "if wandb_api_key:\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    print(\"Logged in to Weights & Biases.\")\n",
    "else:\n",
    "    raise ValueError(\"WANDB_API_KEY not set in .env file.\")\n",
    "\n",
    "# 3. Set your WandB project details and initialize run\n",
    "wandb_project = \"mpesa-llm-finetuning\"\n",
    "wandb_log_model = \"checkpoint\"\n",
    "wandb_watch = \"all\"  # options: \"all\", \"gradients\", \"parameters\", or None\n",
    "\n",
    "# Initialize wandb run with supported arguments only\n",
    "wandb.init(project=wandb_project)\n",
    "wandb.config.update({\"log_model\": wandb_log_model, \"watch\": wandb_watch})\n",
    "print(f\"WandB run initialized: project={wandb_project}, log_model={wandb_log_model}, watch={wandb_watch}\")"
   ],
   "id": "bc3c0b6502bfaafb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Load and Prepare the MPESA SMS Data**\n",
    "\n",
    "In this step, you'll load your pre-processed MPESA SMS dataset from a local file (`output/mpesa_basic.jsonl`). The dataset contains only two fields: `input` (the anonymized SMS) and `output` (the expected extracted information as JSON).\n",
    "\n",
    "We will:\n",
    "- Load the data from the local JSONL file\n",
    "- Randomly split it into training (80%) and test (20%) sets\n",
    "- Format each example into a single text string suitable for supervised fine-tuning (SFT) of a language model\n",
    "\n",
    "**Formatting:**\n",
    "Each example is mapped to the following prompt/response format:\n",
    "\n",
    "```\n",
    "### Task: Extract transaction details from the SMS\n",
    "### Input:\n",
    "<anonymized_sms>\n",
    "### Output:\n",
    "<expected_output_json>\n",
    "```\n",
    "\n",
    "This format helps the model learn to extract structured information from raw SMS text.\n",
    "\n",
    "**Data File Check:**\n",
    "> ⚠️ Before proceeding, make sure `output/mpesa_basic.jsonl` exists and is not empty. If missing, run your data preparation notebook or script to generate it. If the file is empty, check your data pipeline for issues.\n"
   ],
   "id": "ae7563e8baefac12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "# Check for data file existence and non-emptiness\n",
    "DATA_PATH = \"../output/mpesa_basic.jsonl\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Data file not found: {DATA_PATH}\\nTip: Run your data preparation notebook or script to generate it.\")\n",
    "if os.path.getsize(DATA_PATH) == 0:\n",
    "    raise ValueError(f\"Data file is empty: {DATA_PATH}\\nTip: Check your data pipeline for issues.\")\n",
    "\n",
    "# Load the raw data (input/output fields only)\n",
    "raw = load_dataset(\"json\", data_files=DATA_PATH)\n",
    "\n",
    "# Since the data is not pre-split, we split it here (80% train, 20% test)\n",
    "full_ds = raw[\"train\"]\n",
    "split = full_ds.train_test_split(test_size=0.2, seed=42)\n",
    "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(f\"Loaded {len(full_ds)} examples. Split: {len(train_ds)} train, {len(val_ds)} test.\")\n",
    "\n",
    "# Format each example for SFT (Supervised Fine-Tuning)\n",
    "def format_example_basic(ex):\n",
    "    return {\n",
    "        \"text\": (\n",
    "            \"### Task: Extract transaction details from the SMS\\n\"\n",
    "            f\"### Input:\\n{ex['input']}\\n\"\n",
    "            \"### Output:\\n\" + ex[\"output\"]\n",
    "        )\n",
    "    }\n",
    "\n",
    "train_text = train_ds.map(format_example_basic)\n",
    "val_text   = val_ds.map(format_example_basic)\n",
    "\n",
    "# Inspect one example\n",
    "print(\"Sample formatted training example:\\n\", train_text[0][\"text\"][:400])"
   ],
   "id": "638d4109b761a4b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Choose a Base Model**\n",
    "\n",
    "In this step, you'll select a pre-trained language model to fine-tune on your MPESA SMS data. Since you're training on a Mac M1 with 16GB RAM, it's important to pick a model that fits comfortably in memory—ideally with 3 billion parameters or fewer. Smaller models are faster to train and less likely to run into memory issues on consumer hardware.\n",
    "\n",
    "**Recommended options:**\n",
    "- `TinyLlama/TinyLlama-1.1B-Chat-v1.0`\n",
    "  *Very lightweight and ideal for proof-of-concept runs.*\n",
    "- `microsoft/Phi-3-mini-4k-instruct`\n",
    "  *About 3.8B parameters; may require batch_size=1 due to memory limits.*\n",
    "- `Qwen2-1.5B-Instruct`\n",
    "  *A solid small instruction-tuned model.*\n",
    "\n",
    "**Tip:** For best reliability on 16GB RAM, start with TinyLlama. You can always try larger models later if you need more capability and have enough memory.\n",
    "\n",
    "> ⚠️ **Warning:** If you change `MODEL_ID` to a larger model, you may run out of memory on a Mac M1 (16GB). Always monitor your RAM usage and reduce batch size or sequence length if you encounter memory errors.\n"
   ],
   "id": "74b53661ec6b29d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
   "id": "300ed17e2add9cb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Load Tokenizer and Model**\n",
    "\n",
    "In this step, you'll load the tokenizer and base model you selected. This also sets the padding token (to avoid warnings) and ensures the model runs efficiently on your Mac's Apple GPU (MPS).\n",
    "\n",
    "**Key parameters:**\n",
    "- `torch_dtype`: Set to `float16` to reduce memory usage on MPS (Apple Silicon).\n",
    "- `device_map=\"auto\"`: Lets Accelerate automatically place the model on the MPS device for best performance.\n",
    "\n",
    "This setup helps you train larger models within your Mac's memory limits and speeds up computation using the GPU."
   ],
   "id": "4a316d31ed36c3da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Choose the best dtype for your device: float16 for MPS (Apple Silicon), else float32\n",
    "if torch.backends.mps.is_available():\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "# Set the padding token only if not already set (avoids warnings)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model with the appropriate dtype and device placement\n",
    "default_device_map = \"auto\"  # Lets Accelerate place the model on the best device\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=default_device_map,\n",
    ")\n",
    "\n",
    "# Optional: verify device placement\n",
    "print(\"Tokenizer and model loaded successfully.\")\n",
    "print(\"Model device:\", next(model.parameters()).device)"
   ],
   "id": "9b15da0a3d92ce59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Configure LoRA (PEFT)**\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is a method to fine-tune language models with significantly fewer parameters, making the process faster and requiring less memory. It achieves this by freezing the original model weights and adding trainable low-rank matrices.\n",
    "\n",
    "In this step, you'll configure the LoRA settings for your model. These settings control how LoRA is applied during fine-tuning.\n",
    "\n",
    "**Key parameters:**\n",
    "- `r`: The rank of the low-rank matrices. Common values are 4, 8, 16, etc. Higher values allow the model to learn more complex adaptations but require more memory.\n",
    "- `lora_alpha`: A scaling factor for the LoRA parameters. Typical values range from 16 to 32.\n",
    "- `lora_dropout`: The dropout rate for the LoRA layers. Helps prevent overfitting. Common values are 0.1, 0.2, etc.\n",
    "- `bias`: Specifies how to handle bias terms. \"none\" means no bias adaptation. \"all\" adapts all bias terms (uses more memory). \"lora_only\" adapts only biases in LoRA layers. For most cases, \"none\" is recommended.\n",
    "- `task_type`: Set to \"CAUSAL_LM\" for causal language modeling tasks (like GPT, Llama). Use other values for different tasks (see PEFT docs).\n",
    "- `target_modules`: Specifies which model layers to apply LoRA to. Common choices for transformers are `q_proj`, `k_proj`, `v_proj`, and `o_proj`.\n",
    "\n",
    "**What are `q_proj`, `k_proj`, `v_proj`, and `o_proj`?**\n",
    "- These are the main linear projection layers inside the self-attention mechanism of transformer models.\n",
    "    - `q_proj` (Query Projection): Projects input to the query vectors (\"what to look for\").\n",
    "    - `k_proj` (Key Projection): Projects input to the key vectors (\"what is available\").\n",
    "    - `v_proj` (Value Projection): Projects input to the value vectors (\"what information to use\").\n",
    "    - `o_proj` (Output Projection): Projects the attended output back to the model's hidden size (\"integrate attended info\").\n",
    "- Adapting `q_proj` and `v_proj` is often sufficient for most tasks. Including `k_proj` and `o_proj` gives more adaptation capacity but uses more memory and compute.\n",
    "\n",
    "**Tip:** Start with the recommended settings and adjust only if you encounter memory issues or want to experiment with different ranks or modules.\n",
    "\n",
    "> 💡 **Tip:** If you get an error about `target_modules` (e.g., a module not found), you can check available modules by inspecting your model's named modules:\n",
    ">\n",
    "> ```python\n",
    "> for name, module in model.named_modules():\n",
    ">     print(name)\n",
    "> ```\n",
    "> Then update `target_modules` accordingly.\n"
   ],
   "id": "6834676ac93bc007"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # expand if needed: \"k_proj\", \"o_proj\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()"
   ],
   "id": "f65fe77a07b098ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Training Configuration (TRL SFT)**\n",
    "\n",
    "This section sets all the key training parameters (\"trainer knobs\") for supervised fine-tuning (SFT) using the TRL library. Each parameter is explained below, including recommended values and their implications:\n",
    "\n",
    "**Parameter explanations:**\n",
    "- `output_dir`: Directory where model checkpoints and logs will be saved.\n",
    "- `num_train_epochs`: Number of times to iterate over the entire training dataset. More epochs can improve learning but may cause overfitting.\n",
    "- `per_device_train_batch_size`: Number of samples per batch on each device. **Lower values** (e.g., 1) reduce memory usage (recommended for Mac M1). **Higher values** speed up training if you have more memory.\n",
    "- `per_device_eval_batch_size`: Batch size for evaluation. Set to 1 for low memory environments.\n",
    "- `gradient_accumulation_steps`: Number of steps to accumulate gradients before updating model weights. **Increase** this to simulate larger batch sizes without increasing memory usage.\n",
    "- `learning_rate`: Step size for updating model weights. **Typical range:** 2e-4 to 5e-5 for LoRA on small models. **Higher values** speed up learning but may cause instability. **Lower values** are safer but slower.\n",
    "- `warmup_ratio`: Fraction of total steps used for learning rate warmup. Helps stabilize early training.\n",
    "- `lr_scheduler_type`: Learning rate schedule. \"cosine\" is common for smooth decay.\n",
    "- `weight_decay`: Regularization to prevent overfitting. Typical values: 0.01–0.1.\n",
    "- `logging_steps`: How often to log training metrics (in steps).\n",
    "- `save_steps`: How often to save model checkpoints (in steps).\n",
    "- `save_total_limit`: Maximum number of checkpoints to keep. Older ones are deleted.\n",
    "- `max_seq_length`: Maximum sequence length for input data. **SMS are short—256 is plenty.** Reduce to 128 if you hit memory issues.\n",
    "- `fp16`: Use 16-bit floating point precision for training. **Set True on MPS (Apple Silicon) for speed and memory savings.**\n",
    "- `bf16`: Use bfloat16 precision. **Set False on MPS.**\n",
    "- `report_to`: Where to log metrics. Set to \"wandb\" to use Weights & Biases.\n",
    "- `eval_strategy`: When to run evaluation. \"steps\" means evaluate every `eval_steps` steps.\n",
    "- `eval_steps`: How often to run evaluation (in steps).\n",
    "- `dataset_text_field`: Name of the text field in your dataset (should match your data mapping).\n",
    "\n",
    "**Tips:**\n",
    "- If you hit memory errors, try increasing `gradient_accumulation_steps` (e.g., 16) and/or reducing `max_seq_length` (e.g., 128).\n",
    "- Monitor training and validation loss to avoid overfitting (reduce epochs or increase weight_decay if needed).\n",
    "- Adjust `learning_rate` and `batch_size` based on your hardware and dataset size."
   ],
   "id": "ff916935f4a17788"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "train_args = SFTConfig(\n",
    "    output_dir=\"./mpesa-llm-mps\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    max_length=256,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    report_to=\"wandb\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    dataset_text_field=\"text\",  # we mapped to this field\n",
    ")\n"
   ],
   "id": "bd758b091779506e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "(Optional but Useful) **Train-on-Answer Only**\n",
    "\n",
    "This option configures the trainer to compute loss **only on the target/output portion** of each example (the JSON after `### Output:` in your prompt). This is especially useful for instruction-following or extraction tasks, as it:\n",
    "- Focuses learning on the answer, not the prompt.\n",
    "- Usually improves training stability and output quality.\n",
    "- Reduces the risk of the model \"learning\" to copy the prompt or instruction.\n",
    "\n",
    "**Parameter explanations:**\n",
    "- `response_template`: The string that marks the start of the answer in your formatted data (must match your prompt formatting, e.g., `\"### Output:\\n\"`).\n",
    "- `tokenizer`: The tokenizer used for your model. Ensures the data collator can properly split prompt and answer.\n",
    "- `DataCollatorForCompletionOnlyLM`: A special data collator from TRL that masks out the prompt tokens so loss is only computed on the answer tokens.\n",
    "\n",
    "**Tips:**\n",
    "- Make sure `response_template` matches exactly how you formatted your data (including newlines).\n",
    "- This approach is recommended for most extraction, question-answering, and instruction-tuning tasks.\n",
    "\n",
    "> ⚠️ **Assertion:** The collator will check that at least one answer token is found in each batch. If not, it will raise an error. If you see this, check your `response_template` and data formatting."
   ],
   "id": "f4ae6f1bb67fbfe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from trl.trainer.data_collator import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# This must match the delimiter used in formatting above\n",
    "response_template = \"### Output:\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "def assert_answer_tokens(batch):\n",
    "    # Check that at least one answer token is found in each batch\n",
    "    labels = batch[\"labels\"] if \"labels\" in batch else None\n",
    "    if labels is not None:\n",
    "        # -100 is the ignore index; answer tokens are not -100\n",
    "        has_answer = (labels != -100).any().item() if hasattr(labels, 'any') else any(l != -100 for l in labels)\n",
    "        assert has_answer, \"No answer tokens found in batch! Check your response_template and data formatting.\"\n",
    "    return batch\n"
   ],
   "id": "81085278172a794e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Fine‑Tune (SFTTrainer)**\n",
    "\n",
    "This step runs supervised fine-tuning (SFT) with LoRA using the TRL SFTTrainer. Here’s what each parameter does and how to tune them:\n",
    "\n",
    "**Parameter explanations:**\n",
    "- `model`: The model to be fine-tuned. Should already have LoRA adapters applied.\n",
    "- `tokenizer`: The tokenizer for your model. Ensures correct tokenization and padding.\n",
    "- `args`: The training configuration (SFTConfig) containing all training hyperparameters (see previous cell for details).\n",
    "- `train_dataset`: The dataset used for training. Should be preprocessed and formatted as required.\n",
    "- `eval_dataset`: The dataset used for evaluation/validation during training.\n",
    "- `data_collator`: (Optional) Controls how batches are created. If using `DataCollatorForCompletionOnlyLM`, loss is computed only on the answer portion. If omitted, loss is computed on the full prompt+answer.\n",
    "- `compute_metrics`: (Optional) A function to compute custom metrics during evaluation.\n",
    "- `callbacks`: (Optional) List of callback functions for custom training/evaluation hooks.\n",
    "\n",
    "**Tips:**\n",
    "- Use `data_collator=collator` to focus loss on the answer only (recommended for extraction/instruction tasks).\n",
    "- Monitor training and validation loss to check for overfitting or underfitting.\n",
    "- Adjust `args` (batch size, learning rate, epochs, etc.) as needed for your hardware and dataset size.\n",
    "- You can add custom callbacks or metrics for advanced monitoring or early stopping.\n",
    "\n",
    "This setup ensures efficient, targeted fine-tuning of your LLM with LoRA on your MPESA SMS data."
   ],
   "id": "8c32bed470dc31d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_args,\n",
    "    train_dataset=train_text,\n",
    "    eval_dataset=val_text,\n",
    "    data_collator=lambda batch: assert_answer_tokens(collator(batch)),  # check answer tokens\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "b0b86fb1f87852aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Save, Push to Hub, and (Optionally) Merge LoRA**\n",
    "\n",
    "This step covers how to save your fine-tuned adapters, push them to the Hugging Face Hub, and (optionally) merge LoRA weights into a standalone model for easier deployment.\n",
    "\n",
    "**Parameter explanations and workflow:**\n",
    "- `trainer.model.push_to_hub(adapter_repo, private=True)`: Uploads your LoRA adapters to your Hugging Face repository. Set `private=False` if you want the repo to be public.\n",
    "- `trainer.save_model(\"./adapters\")`: Saves the LoRA adapters locally for backup or offline use.\n",
    "- `merge_and_unload()`: Merges the LoRA adapters into the base model weights, producing a single model file. This is useful for exporting a standalone checkpoint for inference on other platforms or stacks.\n",
    "- `base.load_state_dict(model.state_dict(), strict=False)`: Loads the fine-tuned LoRA weights into the base model before merging.\n",
    "- `merged.save_pretrained(\"./mpesa-merged\")`: Saves the merged, standalone model locally.\n",
    "\n",
    "**When should you merge?**\n",
    "- **Merge** if you want a single checkpoint for deployment or inference outside the PEFT/LoRA ecosystem (e.g., for ONNX export, or use in other frameworks).\n",
    "- **Do not merge** if you want to keep the adapters lightweight and flexible for further fine-tuning or experimentation. Keeping adapters separate is more memory-efficient and allows for easy swapping or stacking of adapters.\n",
    "\n",
    "**Tips:**\n",
    "- Always save both the adapters and (if needed) the merged model for maximum flexibility.\n",
    "- Use descriptive repo names and local paths to keep track of your experiments.\n",
    "- If you plan to share your model, make sure to push to a public repo or set `private=False`.\n",
    "\n",
    "> 💡 **Tip:** Make sure you have write access to the Hugging Face repo before pushing. If you get a permissions error, check your repo settings and your HF token scopes.\n",
    ">\n",
    "> 💡 **Tip:** When merging LoRA weights, ensure the base model and LoRA config match exactly (same model architecture, LoRA rank, and target modules). Mismatches can cause errors or degraded performance.\n"
   ],
   "id": "bddb8af7dedd3d16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Push adapters to your HF repo\n",
    "adapter_repo = \"your-username/mpesa-tinyllama-lora\"\n",
    "trainer.model.push_to_hub(adapter_repo, private=True)\n",
    "\n",
    "# Save locally too\n",
    "trainer.save_model(\"./adapters\")\n",
    "\n",
    "# (Optional) Merge LoRA into base weights for easier inference export\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Reload base model, apply adapters, then merge\n",
    "base = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=dtype, device_map=\"auto\")\n",
    "base = get_peft_model(base, lora_cfg)\n",
    "base.load_state_dict(model.state_dict(), strict=False)\n",
    "merged = base.merge_and_unload()\n",
    "merged.save_pretrained(\"./mpesa-merged\")"
   ],
   "id": "8be93c40d26bf1ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Quick Sanity‑Check Inference**\n",
    "\n",
    "This step runs a quick inference to verify that your fine-tuned model produces the expected JSON output for a sample MPESA SMS. This is a practical way to check if your model is working as intended before deploying or running a full evaluation.\n",
    "\n",
    "**Parameter explanations:**\n",
    "- `pipeline`: The Hugging Face pipeline for text generation. Handles tokenization, model inference, and decoding.\n",
    "- `model`: The trained model to use for inference. Here, it's the LoRA‑wrapped model from training.\n",
    "- `tokenizer`: The tokenizer used for your model. Ensures input is properly tokenized and output is decoded.\n",
    "- `device_map`: Controls which device(s) to use for inference. \"auto\" lets Accelerate pick the best device (MPS/CPU/GPU).\n",
    "- `torch_dtype`: Data type for inference. Use `float16` on MPS for speed and memory savings.\n",
    "- `sample`: The formatted prompt for inference. Should match your training prompt structure.\n",
    "- `max_new_tokens`: Maximum number of tokens to generate in the output. Increase if your expected JSON is long.\n",
    "- `do_sample`: Whether to use sampling (randomness) in generation. `False` means deterministic output (recommended for extraction tasks).\n",
    "\n",
    "**Tips:**\n",
    "- Always use a prompt that matches your training format for best results.\n",
    "- If the output is truncated, increase `max_new_tokens`.\n",
    "- For more robust evaluation, try several real SMS examples and compare outputs to expected JSON.\n",
    "- If you see hallucinated or incomplete outputs, consider further fine-tuning or prompt engineering.\n",
    "\n",
    "This quick check gives you confidence that your model is extracting information as expected before moving to production or sharing results.\n",
    "\n",
    "> ⚠️ **Sanity-Check Output:** The code below checks the output length and warns if the output is empty or likely truncated. Adjust `max_new_tokens` if needed."
   ],
   "id": "2e1b02ba1b5c71cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=trainer.model,  # uses LoRA‑wrapped model\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "\n",
    "sample = (\n",
    "    \"### Task: Extract transaction details from the SMS\\n\"\n",
    "    \"### Input:\\nQFC3D45G7 Confirmed. You have received Ksh500 from Customer_1 XXXXXXX on 12/08/24 at 11:32 AM. New M-PESA balance is Ksh1,250.\\n\"\n",
    "    \"### Output:\\n\"\n",
    ")\n",
    "\n",
    "out = gen(sample, max_new_tokens=150, do_sample=False)\n",
    "output_text = out[0][\"generated_text\"][len(sample):]\n",
    "print(output_text)\n",
    "\n",
    "if not output_text.strip():\n",
    "    print(\"⚠️ Warning: Output is empty. Check your model, prompt, or try increasing max_new_tokens.\")\n",
    "elif len(output_text) >= 140:\n",
    "    print(\"⚠️ Warning: Output may be truncated. Try increasing max_new_tokens for longer outputs.\")\n"
   ],
   "id": "a3e104e8f8a9bfbb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
