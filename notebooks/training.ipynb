{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **MPESA LLM Fineâ€‘Tuning on Mac M1 (16GB): Step-by-Step Guide**\n",
    "\n",
    "This notebook is a practical, beginner-friendly guide for fine-tuning a Large Language Model (LLM) using MPESA SMS transaction data. Each step is clearly explained, with code and rationale, so you can follow along and understand the process from start to finish.\n",
    "\n",
    "**What you'll accomplish:**\n",
    "- Prepare and load your MPESA SMS dataset\n",
    "- Select and configure a base LLM\n",
    "- Apply LoRA (PEFT) for efficient fine-tuning\n",
    "- Set up and run supervised fine-tuning (SFT) with TRL\n",
    "- Save, push, and optionally merge your trained model\n",
    "- Run a quick inference to check your results\n",
    "\n",
    "**Workflow Overview:**\n",
    "1. Login to Hugging Face and Weights & Biases\n",
    "2. Load and Prepare the MPESA SMS Data\n",
    "3. Choose a Base Model\n",
    "4. Load Tokenizer and Model\n",
    "5. Configure LoRA (PEFT)\n",
    "6. Training Configuration (TRL SFT)\n",
    "7. Train-on-Answer (ToA)\n",
    "8. Fine-Tune (SFT Trainer)\n",
    "9. Save, Push to Hub, and Optionally Merge LoRA Weights\n",
    "10. Quick Sanity Check Inference"
   ],
   "id": "470ffd64b5d6a0e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. **Login to Hugging Face and Weights & Biases**",
   "id": "61f759ecd78669a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Check for .env file and load environment variables\n",
    "if not os.path.exists('.env'):\n",
    "    print(\"Warning: .env file not found in the current directory.\")\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# 1. Login to Hugging Face (run this once per session)\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"Logged in to Hugging Face Hub.\")\n",
    "else:\n",
    "    raise ValueError(\"HF_TOKEN not set in .env file.\")\n",
    "\n",
    "# 2. Login to Weights & Biases (run this once per session)\n",
    "if wandb_api_key:\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    print(\"Logged in to Weights & Biases.\")\n",
    "else:\n",
    "    raise ValueError(\"WANDB_API_KEY not set in .env file.\")\n",
    "\n",
    "# 3. Set your WandB project details and initialize run\n",
    "wandb_project = \"mpesa-llm-finetuning\"\n",
    "wandb_log_model = \"checkpoint\"\n",
    "wandb_watch = \"all\"  # options: \"all\", \"gradients\", \"parameters\", or None\n",
    "\n",
    "wandb.init(project=wandb_project, log_model=wandb_log_model, watch=wandb_watch)\n",
    "print(f\"WandB run initialized: project={wandb_project}, log_model={wandb_log_model}, watch={wandb_watch}\")"
   ],
   "id": "bc3c0b6502bfaafb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
